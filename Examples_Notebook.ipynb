{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My actual submission is based on pandas due to the relatively small size of the data but \n",
    "#I know that Pyspark  is popular so i have prepared a pyspark alternate \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, udf, lag, lit\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.window import Window\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_data():\n",
    "    \"\"\"Fetch data from the API and return as a list of dictionaries.\"\"\"\n",
    "    offset = 0\n",
    "    batch_size = 1000\n",
    "    continue_fetching = True\n",
    "    all_data = []\n",
    "    \n",
    "    while continue_fetching:\n",
    "        try:\n",
    "            url = f\"https://healthdata.gov/resource/g62h-syeh.json?$offset={offset}\"\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if not data:\n",
    "                continue_fetching = False\n",
    "            else:\n",
    "                all_data.extend(data)\n",
    "                offset += batch_size\n",
    "\n",
    "        except HTTPError as http_err:\n",
    "            logging.error(f\"HTTP error occurred: {http_err}\")\n",
    "            raise\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Other error occurred: {err}\")\n",
    "            raise\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "# UDF for normalization with parrallelization\n",
    "@udf(FloatType())\n",
    "def normalize_data(value, min_value, max_value):\n",
    "    return (value - min_value) / (max_value - min_value) if max_value > min_value else 0\n",
    "\n",
    "def clean_process_pyspark():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    \n",
    "    # Fetch data\n",
    "    # Data fetching cannot be easily parrallelized so seperate it from the spark parts \n",
    "    data = fetch_data()\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Hire Aquil Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "    df = spark.createDataFrame(data)\n",
    "\n",
    "    keeping_cols = [\n",
    "        'state', 'date', 'inpatient_beds', 'inpatient_beds_used', 'staffed_adult_icu_bed_occupancy',\n",
    "        'inpatient_beds_used_covid', 'inpatient_beds_used_covid_coverage', 'critical_staffing_shortage_today_yes',\n",
    "        'critical_staffing_shortage_today_no', 'hospital_onset_covid', 'previous_day_admission_adult_covid_confirmed',\n",
    "        'total_staffed_adult_icu_beds', 'hospital_onset_covid_coverage', 'total_patients_hospitalized_confirmed_influenza_and_covid',\n",
    "        'total_patients_hospitalized_confirmed_influenza', 'deaths_covid', 'previous_day_deaths_influenza'\n",
    "    ]\n",
    "\n",
    "    working_df= df.select(*keeping_cols)\n",
    "\n",
    "\n",
    "    # Convert and clean data\n",
    "    working_df = working_df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "    numeric_cols = [\n",
    "        \"inpatient_beds\", \"inpatient_beds_used\", \"staffed_adult_icu_bed_occupancy\",\n",
    "        \"inpatient_beds_used_covid\", \"critical_staffing_shortage_today_yes\",\n",
    "        \"critical_staffing_shortage_today_no\", \"hospital_onset_covid\",\n",
    "        \"previous_day_admission_adult_covid_confirmed\", \"total_staffed_adult_icu_beds\",\n",
    "        \"total_patients_hospitalized_confirmed_influenza_and_covid\"\n",
    "    ]\n",
    "    \n",
    "    for col_name in numeric_cols:\n",
    "        working_df = working_df.withColumn(col_name, col(col_name).cast('float'))\n",
    "\n",
    "    # Defining Window\n",
    "    windowSpec = Window.partitionBy(\"state\").orderBy(\"date\")\n",
    "    \n",
    "    # Applying differencing\n",
    "    working_df = working_df.withColumn(\"inpatient_beds_used_diff\", \n",
    "                       col(\"inpatient_beds_used\") - lag(\"inpatient_beds_used\", 14).over(windowSpec))\n",
    "\n",
    "\n",
    "\n",
    "    normalize_udf = udf(normalize_data, FloatType())\n",
    "    \n",
    "    # Apply normalization using UDF\n",
    "    for col_name in numeric_cols:\n",
    "        min_col = working_df.agg({col_name: 'min'}).collect()[0][0]\n",
    "        max_col = working_df.agg({col_name: 'max'}).collect()[0][0]\n",
    "        working_df = working_df.withColumn(f'normalized_{col_name}', normalize_udf(col(col_name), lit(min_col), lit(max_col)))\n",
    "    \n",
    "    # Show example data\n",
    "    #working_df.show()\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_process_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_data_pandas():\n",
    "    \"\"\"Fetches healthcare data iteratively and combines into a single DataFrame then cleans it using Pandas.\n",
    "        returns a pandas dataframe\n",
    "    \"\"\"\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Initialize DataFrame to accumulate the results\n",
    "    raw_df = pd.DataFrame()\n",
    "    \n",
    "    # Initial offset and batch size\n",
    "    offset = 0\n",
    "    batch_size = 1000\n",
    "    continue_fetching = True\n",
    "    time_loaded = datetime.now()\n",
    "\n",
    "    while continue_fetching:\n",
    "        try:\n",
    "            # Fetch data from API\n",
    "            url = f\"https://healthdata.gov/resource/g62h-syeh.json?$offset={offset}\"\n",
    "            logging.info(f\"Retrieving data with offset {offset}\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises HTTPError for bad responses\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            # Check if data is empty to decide whether to continue\n",
    "            if not data:\n",
    "                continue_fetching = False\n",
    "                logging.info(\"All data collected.\")\n",
    "            else:\n",
    "                # create and append dataframe\n",
    "                current_df = pd.DataFrame(data)\n",
    "                raw_df = pd.concat([raw_df, current_df], ignore_index=True)\n",
    "\n",
    "                # Increment the offset\n",
    "                offset += batch_size\n",
    "\n",
    "        except HTTPError as http_err:\n",
    "            logging.error(f\"HTTP error occurred: {http_err}\")  # Log HTTP error\n",
    "            raise\n",
    "        except Exception as err:\n",
    "            logging.error(f\"Other error occurred: {err}\")  # Log other errors that may occur\n",
    "            raise\n",
    "\n",
    "    # Data cleaning\n",
    "    if not raw_df.empty:\n",
    "        logging.info(\"Begin data processing\")\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Define columns to keep\n",
    "        keeping_cols = [\n",
    "            'state', 'date', 'inpatient_beds', 'inpatient_beds_used', 'staffed_adult_icu_bed_occupancy',\n",
    "            'inpatient_beds_used_covid', 'inpatient_beds_used_covid_coverage', 'critical_staffing_shortage_today_yes',\n",
    "            'critical_staffing_shortage_today_no', 'hospital_onset_covid', 'previous_day_admission_adult_covid_confirmed',\n",
    "            'total_staffed_adult_icu_beds', 'hospital_onset_covid_coverage', 'total_patients_hospitalized_confirmed_influenza_and_covid',\n",
    "            'total_patients_hospitalized_confirmed_influenza', 'deaths_covid', 'previous_day_deaths_influenza'\n",
    "        ]\n",
    "        \n",
    "        # Define colums for conversion\n",
    "        numeric_cols = [\n",
    "            'inpatient_beds', 'inpatient_beds_used', 'staffed_adult_icu_bed_occupancy', 'inpatient_beds_used_covid',\n",
    "            'inpatient_beds_used_covid_coverage', 'critical_staffing_shortage_today_yes', 'critical_staffing_shortage_today_no',\n",
    "            'hospital_onset_covid', 'previous_day_admission_adult_covid_confirmed', 'total_staffed_adult_icu_beds',\n",
    "            'hospital_onset_covid_coverage', 'total_patients_hospitalized_confirmed_influenza_and_covid',\n",
    "            'total_patients_hospitalized_confirmed_influenza', 'deaths_covid', 'previous_day_deaths_influenza'\n",
    "        ]\n",
    "        \n",
    "        # Minimal cleaning and deleting cols also protects against new columns being added unexpectedly\n",
    "        cleaned = raw_df[keeping_cols].dropna(how='all')\n",
    "\n",
    "        # Convert fields\n",
    "        cleaned['date'] = pd.to_datetime(cleaned['date'])\n",
    "        cleaned[numeric_cols] = cleaned[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # State-wise median filling and normalization\n",
    "        cleaned[numeric_cols] = cleaned.groupby('state')[numeric_cols].transform(lambda x: x.fillna(x.median()))\n",
    "        cleaned['normalized_inpatient_beds_used'] = cleaned.groupby('state')['inpatient_beds_used'].transform(\n",
    "            lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten()\n",
    "        )\n",
    "        \n",
    "        # Differencing for time series data\n",
    "        cleaned.set_index('date', inplace=True)\n",
    "        cleaned.sort_index(inplace=True)\n",
    "        cleaned['inpatient_beds_used_diff'] = cleaned.groupby('state')['inpatient_beds_used'].diff(periods=14)        \n",
    "        cleaned.reset_index(inplace=True)\n",
    "        cleaned['time_loaded'] = time_loaded  # Adding time loaded for data traceability\n",
    "\n",
    "        return cleaned\n",
    "    else:\n",
    "        logging.info(\"No data was collected.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched=fetch_data_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full airflow example\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['aquil.codes@gmail.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "#Intitlaizing Dag and grabbing data on Wednesday and Friday when it is updated\n",
    "@dag(default_args=default_args, schedule_interval='0 1 * * 3,5', start_date=days_ago(1), catchup=False)\n",
    "def healthcare_data_processing_flow():\n",
    "    \n",
    "    @task\n",
    "    def fetch_data():\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        raw_df = pd.DataFrame()\n",
    "        offset = 0\n",
    "        batch_size = 1000\n",
    "        continue_fetching = True\n",
    "        api_url = \"https://healthdata.gov/resource/g62h-syeh.json\"\n",
    "        \n",
    "        while continue_fetching:\n",
    "            try:\n",
    "                url = f\"{api_url}?$offset={offset}\"\n",
    "                logging.info(f\"Retrieving data with offset {offset}\")\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "\n",
    "                if not data:\n",
    "                    continue_fetching = False\n",
    "                    logging.info(\"All data collected.\")\n",
    "                else:\n",
    "                    current_df = pd.DataFrame(data)\n",
    "                    raw_df = pd.concat([raw_df, current_df], ignore_index=True)\n",
    "                    offset += batch_size\n",
    "            except HTTPError as http_err:\n",
    "                logging.error(f\"HTTP error occurred: {http_err}\")\n",
    "                raise\n",
    "            except Exception as err:\n",
    "                logging.error(f\"Other error occurred: {err}\")\n",
    "                raise\n",
    "        return raw_df\n",
    "\n",
    "    @task\n",
    "    def transform_data(raw_df):\n",
    "        if raw_df.empty:\n",
    "            logging.info(\"No data was collected.\")\n",
    "            return None\n",
    "\n",
    "        logging.info(\"Begin data processing\")\n",
    "        time_transformed = datetime.now()\n",
    "        scaler = StandardScaler()\n",
    "        keeping_cols = [\n",
    "            'state', 'date', 'inpatient_beds', 'inpatient_beds_used', 'staffed_adult_icu_bed_occupancy',\n",
    "            'inpatient_beds_used_covid', 'inpatient_beds_used_covid_coverage', 'critical_staffing_shortage_today_yes',\n",
    "            'critical_staffing_shortage_today_no', 'hospital_onset_covid', 'previous_day_admission_adult_covid_confirmed',\n",
    "            'total_staffed_adult_icu_beds', 'hospital_onset_covid_coverage', 'total_patients_hospitalized_confirmed_influenza_and_covid',\n",
    "            'total_patients_hospitalized_confirmed_influenza', 'deaths_covid', 'previous_day_deaths_influenza'\n",
    "        ]\n",
    "        \n",
    "        # Define colums for conversion\n",
    "        numeric_cols = [\n",
    "            'inpatient_beds', 'inpatient_beds_used', 'staffed_adult_icu_bed_occupancy', 'inpatient_beds_used_covid',\n",
    "            'inpatient_beds_used_covid_coverage', 'critical_staffing_shortage_today_yes', 'critical_staffing_shortage_today_no',\n",
    "            'hospital_onset_covid', 'previous_day_admission_adult_covid_confirmed', 'total_staffed_adult_icu_beds',\n",
    "            'hospital_onset_covid_coverage', 'total_patients_hospitalized_confirmed_influenza_and_covid',\n",
    "            'total_patients_hospitalized_confirmed_influenza', 'deaths_covid', 'previous_day_deaths_influenza'\n",
    "        ]\n",
    "        \n",
    "        # Minimal cleaning and deleting cols also protects against new columns being added unexpectedly\n",
    "        cleaned = raw_df[keeping_cols].dropna(how='all')\n",
    "\n",
    "        # Convert fields\n",
    "        cleaned['date'] = pd.to_datetime(cleaned['date'])\n",
    "        cleaned[numeric_cols] = cleaned[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # State-wise median filling and normalization\n",
    "        cleaned[numeric_cols] = cleaned.groupby('state')[numeric_cols].transform(lambda x: x.fillna(x.median()))\n",
    "        cleaned['normalized_inpatient_beds_used'] = cleaned.groupby('state')['inpatient_beds_used'].transform(\n",
    "            lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten()\n",
    "        )\n",
    "        \n",
    "        # Differencing for time series data\n",
    "        cleaned.set_index('date', inplace=True)\n",
    "        cleaned.sort_index(inplace=True)\n",
    "        cleaned['inpatient_beds_used_diff'] = cleaned.groupby('state')['inpatient_beds_used'].diff(periods=14)        \n",
    "        cleaned.reset_index(inplace=True)\n",
    "        cleaned['time_transformed'] = time_transformed  # Adding time loaded for data lineage\n",
    "\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    @task\n",
    "    def load_to_db(cleaned_df):\n",
    "        if cleaned_df is not None:\n",
    "            conn = sqlite3.connect('health_data.db')\n",
    "            cleaned_df.to_sql('healthcare_data', conn, if_exists='replace', index=False)\n",
    "            conn.close()\n",
    "\n",
    "    # Task dependencies\n",
    "    raw_data = fetch_data()\n",
    "    cleaned_data = transform_data(raw_data)\n",
    "    load_to_db(cleaned_data)\n",
    "\n",
    "dag = healthcare_data_processing_flow()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello-heart2-IqrgmrSX-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
